# Incident Response Guide - Ubiquitous Tribble Fintech Platform

## Table of Contents
1. [Overview](#overview)
2. [Incident Severity Levels](#incident-severity-levels)
3. [Incident Response Process](#incident-response-process)
4. [Roles & Responsibilities](#roles--responsibilities)
5. [Communication Protocol](#communication-protocol)
6. [Response Playbooks](#response-playbooks)
7. [Post-Incident Review](#post-incident-review)

---

## Overview

This guide defines how the engineering and operations teams respond to incidents affecting the Ubiquitous Tribble payment platform.

**Goals:**
- Minimize customer impact and financial loss
- Restore service to normal operation quickly
- Communicate transparently with stakeholders
- Learn from incidents to prevent recurrence
- Maintain audit trail and compliance documentation

**Scope:**
- Production incidents affecting customers
- Data integrity issues
- Security incidents
- Service unavailability
- Performance degradation >30% from baseline

---

## Incident Severity Levels

### SEV-1: Critical (Page immediately)

**Definition**: Service completely unavailable or major data loss affecting all/most customers

**Examples**:
- All payment processing down
- Database completely unavailable
- Fraudulent access to customer data
- Large-scale customer funds lost
- Data corruption affecting millions of transactions

**Response Time**: 5 minutes to declare incident, have lead on call
**Expected Resolution**: < 1 hour
**Customer Communication**: Every 15 minutes

**On-Call**: All leads immediately
**Communication**: Page + Slack + Phone calls

```
SEV-1 CHECKLIST:
â˜ Declare incident in #incidents Slack channel
â˜ Page all engineering leads
â˜ Get incident commander + tech lead
â˜ Send customer notification within 5 minutes
â˜ Begin root cause investigation
â˜ Executive team briefed
â˜ Customer success team ready for support volume
```

---

### SEV-2: High (Page senior engineer)

**Definition**: Significant service degradation or partial outage affecting 10%+ of customers

**Examples**:
- Subset of payment methods failing (e.g., cards but not bank transfers)
- Settlement processing delayed >4 hours
- High error rate (5-20%)
- API latency >2 seconds p95
- One or two critical merchants down

**Response Time**: 15 minutes to declare incident
**Expected Resolution**: < 4 hours
**Customer Communication**: Every 30 minutes

**On-Call**: On-call engineer + Engineering manager
**Communication**: Slack + email to senior team

```
SEV-2 CHECKLIST:
â˜ Declare incident with "sev-2" tag
â˜ Page on-call engineer
â˜ Notify incident commander
â˜ Assess customer impact (how many, which merchants)
â˜ Update status page
â˜ Notify affected customers directly
â˜ Begin investigation
â˜ Establish resolution timeline
```

---

### SEV-3: Medium (Create ticket, page if ongoing)

**Definition**: Service degradation or outage affecting <10% of customers or non-critical features

**Examples**:
- One merchant account access issues
- Reports/analytics delayed
- Webhook delivery delayed >1 hour
- Dispute submission taking longer than normal
- Rate limiting being too strict

**Response Time**: 1 hour to respond
**Expected Resolution**: < 8 hours
**Customer Communication**: Update ticket with progress

**On-Call**: Engineering team (no page required)
**Communication**: Slack thread

```
SEV-3 CHECKLIST:
â˜ Create incident ticket
â˜ Estimate scope and impact
â˜ Assign to on-call team
â˜ Update status if public-facing
â˜ Monitor until resolved
```

---

### SEV-4: Low (Handle during normal work hours)

**Definition**: Minor issues, no customer impact or very few customers affected

**Examples**:
- Documentation outdated
- Non-critical endpoint slow
- Admin dashboard slow
- Single user experience issue with workaround
- Alert fatigue (false alerts)

**Response Time**: Next business day
**Expected Resolution**: < 5 days
**Customer Communication**: Via ticket response

**On-Call**: None
**Communication**: Email or ticket

```
SEV-4 CHECKLIST:
â˜ Create GitHub issue
â˜ Label with severity and component
â˜ Prioritize in backlog
â˜ Schedule for next sprint if necessary
```

---

## Incident Response Process

### Phase 1: Detection (0-5 minutes)

**Who detects incidents?**
- Automated monitoring/alerting (primary)
- Customer reports via support (secondary)
- Internal systems check (tertiary)

**Detection methods:**
- Prometheus alerts firing
- Datadog anomaly detection
- Error rate thresholds exceeded
- Synthetic monitoring failing
- Customer support spike

**On detection:**
```
1. Verify the incident (not false positive)
   - Check multiple data sources
   - Confirm real customer impact
   - Rule out local/testing environment

2. Initial severity assessment
   - How many customers affected?
   - What percentage of traffic?
   - Is payment processing down?
   - Can customers transact?

3. Declare incident in Slack
   #incidents: "ðŸš¨ SEV-2: Payment processing delays detected. ~15% of transactions delayed >500ms. Investigating. ETC: 30 mins"
```

### Phase 2: Initial Response (5-15 minutes)

**Key actions:**
1. Get incident commander on call
2. Declare severity level officially
3. Brief incident response team
4. Assess scope (which customers, which features)
5. Communicate status to stakeholders

**Incident commander** (someone who has done this before):
- Takes lead on communication
- Coordinates investigation
- Makes major decisions (rollback, etc.)
- Prevents "too many cooks" scenario

**Slack channel setup:**
```
#incidents - Public incident status (customers may be watching)
#incident-sev-2-12345 - Private investigation channel
#incident-exec-sev-2-12345 - Executive updates (for SEV-1/2)
```

**Initial status message:**
```
ðŸš¨ INCIDENT DECLARED: SEV-2 - Payment Processing Delays

**Status**: Investigation in progress
**Impact**:
  - ~15% of transactions experiencing >500ms latency
  - ~500 affected customers
  - ~10,000 delayed transactions/min
**Services affected**: Payment processing (core), unaffected settlement
**Root cause**: Under investigation

**Timeline**:
- 10:15 AM: Alert fired - high p95 latency
- 10:18 AM: Incident declared
- 10:25 AM: Incident commander assigned
- ETC for resolution: 11:00 AM (35 mins)

**Next update**: 10:40 AM (every 15 mins)
**Leads**: @incident-commander, @backend-lead
```

### Phase 3: Investigation & Mitigation (15 mins - resolution)

**Investigation approach:**
1. **Gather data**
   - Check recent deployments
   - Review metrics and logs
   - Check 3rd party service status
   - Interview relevant teams

2. **Form hypothesis**
   - "Database query got slow after last migration"
   - "Rate limiter misconfigured"
   - "Stripe API timeout"

3. **Test hypothesis**
   - Run diagnostic queries
   - Check specific logs
   - Reproduce in staging if possible

4. **Identify root cause**
   - Why did this happen?
   - When did it start?
   - What changed recently?

**Mitigation options (in priority order):**

**Option A: Quick fix (if root cause identified)**
```
Example: Query became slow
Fix: Update database index
- Risk: Low (tested in staging)
- Time: 10 minutes
- Rollback: Simple (remove index)
```

**Option B: Rollback (if caused by recent change)**
```
Rollback to last known good version
- Risk: Customers lose features deployed 2 hours ago
- Time: 5 minutes
- Rollback: Re-deploy old version
- Check: Monitoring for 15 minutes after
```

**Option C: Circuit breaker (if 3rd party down)**
```
Stop calling failing service, use default/cached response
- Risk: Some functionality degraded but system stays up
- Time: 5 minutes
- Rollback: Re-enable calls when service recovers
```

**Option D: Scale resources (if overload)**
```
Add more database replicas, horizontal scaling
- Risk: Cost increase, may not help if CPU bottleneck
- Time: 15-30 minutes
- Rollback: Scale down after incident
```

**Option E: Rate limiting (if attack/overload)**
```
Temporarily limit requests per customer to preserve system
- Risk: Some customers get rate limited
- Time: 5 minutes
- Rollback: Remove limits when stable
```

**Communication during investigation:**
```
Every 15 minutes update status:

ðŸ”„ INVESTIGATING: Payment Processing Delays (SEV-2)

**Latest findings**:
- Root cause likely identified: Database query on payment lookup getting slow
- Hypothesis: Recent merchant index addition caused query optimizer to choose different plan
- Testing: Running query plan analysis
- Mitigation: Plan to update index hints or add index
- Status: Still at full impact (~500 TXN/min affected)

**Next steps**:
1. Validate hypothesis with DBA (ETC: 5 mins)
2. Test fix in staging (ETC: 10 mins)
3. Deploy fix to production (ETC: 5 mins)
4. Monitor for 15 minutes

**ETC full resolution**: 11:15 AM (+30 mins)
**Next update**: 11:00 AM
```

### Phase 4: Remediation (resolution - 1 hour after)

**When incident is resolved:**

1. **Verify fix is working**
   ```
   - Confirm error rate back to normal
   - Confirm latency back to baseline
   - Confirm no new errors introduced
   - Confirm metrics stable for 5 minutes
   ```

2. **Update status**
   ```
   âœ… RESOLVED: Payment Processing Delays (SEV-2)

   **Root cause**: Database query plan changed after recent index addition
   **Duration**: 55 minutes (detected 10:15 AM, resolved 11:10 AM)
   **Impact**: ~27,500 transactions delayed >500ms
   **Fix**: Updated index hints to use original query plan

   **Post-incident timeline**:
   - 11:10 AM: Fix deployed, transactions returning to normal latency
   - 11:15 AM: Confirmed latency back to baseline (<150ms p95)
   - 11:30 AM: Stable for 15+ minutes, marked RESOLVED

   **Post-incident review**: Scheduled for 2024-01-16 at 2:00 PM
   Leads: @incident-commander, @dba-team, @backend-team
   ```

3. **Notify stakeholders**
   ```
   Email to customers (if SEV-1/2):
   Subject: Service Restored - Payment Processing Incident

   Dear valued customer,

   We experienced a brief incident affecting payment processing between
   10:15 AM and 11:10 AM UTC today. The issue has been resolved.

   Impact: ~500 customers experienced payment processing delays. We have
   verified all transactions completed successfully and no funds were lost.

   Root cause: A recent database optimization caused query plan changes
   affecting transaction lookup performance.

   Prevention: Our engineering team is implementing additional monitoring
   for query plan changes and will add automated alerts for this scenario.

   Thank you for your patience. We take reliability seriously.
   ```

### Phase 5: Post-Incident (1-7 days after)

**Mandatory post-incident review (PIR):**
- Scheduled within 24 hours
- Includes everyone involved in incident + senior leadership
- Blameless - focus on process, not people
- Documents: root cause, timeline, lessons learned, action items

**See Post-Incident Review section below**

---

## Roles & Responsibilities

### Incident Commander (IC)
**Role**: Lead and coordinate incident response

**Responsibilities**:
- Declare incident severity
- Assemble response team
- Drive decision-making
- Communicate progress
- Ensure follow-up post-incident

**Who can be IC**:
- Engineering managers and above
- Designated on-call (rotated monthly)
- Senior engineers with IR training (after training)

**Not IC responsibilities** (hand off to specialists):
- Actual system debugging (tech lead)
- Customer communication (product/support)
- Billing/refund decisions (exec)

---

### Technical Lead (Tech Lead)
**Role**: Lead technical investigation and fix

**Responsibilities**:
- Investigate root cause
- Propose mitigation options with trade-offs
- Execute the fix
- Verify system recovered
- Brief incident commander regularly

**Who can be Tech Lead**:
- Any engineer who understands the system
- Can request backup from more experienced engineer

---

### Executive Sponsor (for SEV-1 only)
**Role**: Executive visibility and decision-making

**Responsibilities**:
- Authorize unusual actions (downtime, customer credits)
- Brief customers/partners if needed
- Approve any temporary workarounds
- Attend post-incident review

**Who**:
- VP Engineering
- CTO
- CEO (if >$100K impact)

---

### Customer Communication Lead (for SEV-2+ and if public impact)
**Role**: Communicate with customers

**Responsibilities**:
- Draft customer notifications
- Update status page
- Monitor support tickets
- Prepare refund/credit recommendations

**Who**:
- Support manager
- Product manager

---

## Communication Protocol

### Initial Incident Declaration

**Slack message (in #incidents):**
```
ðŸš¨ INCIDENT DECLARED: [SEVERITY] - [BRIEF DESCRIPTION]

**Impact Summary**:
- Affected customers: [#] out of [total]
- Affected feature: [feature(s)]
- Current status: Investigating | Mitigating | Recovering

**Services**:
- ðŸ”´ Service A (DOWN)
- ðŸŸ¡ Service B (DEGRADED)
- ðŸŸ¢ Service C (HEALTHY)

**Known timeline**:
- [Time]: Alert fired / Issue detected
- [Time]: Incident declared
- [Time]: Investigation started

**Incident Lead**: @incident-commander
**Next update**: [TIME] (every 15 mins for SEV-1/2)
**Link to war room**: [Zoom/Teams link]
```

### Ongoing Updates (every 15 minutes for SEV-1/2, every 30 mins for SEV-3)

```
ðŸ”„ UPDATE: [INCIDENT TITLE]

**Current Status**: [Investigating | Mitigating | Recovering]
**Health**: Still degraded

**Root cause hypothesis**:
[What we think is happening]

**Work in progress**:
1. [Action 1] - ETC: [time]
2. [Action 2] - ETC: [time]
3. [Action 3] - ETC: [time]

**Metrics** (current vs baseline):
- Error rate: 15% (normal: 0.1%) âŒ
- P95 latency: 2.3s (normal: 150ms) âŒ
- Throughput: 8K TXN/min (normal: 12K) âš ï¸

**ETC full recovery**: [time] ([mins remaining])
**Next update**: [time]
```

### Resolution Announcement

```
âœ… INCIDENT RESOLVED: [INCIDENT TITLE]

**Duration**: [X minutes] from detection to resolution
**Impact**:
- Customers affected: [#]
- Transactions delayed/failed: [#]
- Estimated customer impact: [description]

**Root cause**: [Technical explanation]

**What we did**:
1. [Mitigation action 1]
2. [Mitigation action 2]

**What we learned**:
- [Key learning 1]
- [Key learning 2]
- [Prevention we're implementing]

**Post-incident review**: Scheduled [date/time]
**Apology**: "We sincerely apologize for the disruption. We're committed to preventing this in the future."
```

### Customer Notification (for SEV-1/2 with public impact)

**Status page update:**
```
ðŸ”´ INCIDENT: Payment processing delays affecting approximately 15% of transactions

We are investigating payment processing delays. Affected transactions may take longer to process.
We do not believe any transactions have failed - processing is delayed.

Last updated: 10:45 AM UTC | Next update: 11:00 AM UTC
```

**After resolution:**
```
âœ… RESOLVED: Payment processing incident

The incident has been resolved. All transactions that were delayed have now completed successfully.
We apologize for the disruption and appreciate your patience while we investigated and fixed this issue.
```

**Email to affected customers (if needed):**
```
Subject: Update on Payment Processing Incident

We experienced a brief incident affecting payment processing today between
10:15 AM and 11:10 AM UTC.

Details:
- Incident: Database performance issue affecting payment transaction lookup
- Impact: ~500 customers experienced delays of 30 seconds to 2 minutes
- Resolution: We identified the root cause and deployed a fix
- Customer action required: None - all transactions have completed

No funds were lost and all pending transactions completed successfully.

We deeply apologize for this incident. This is not the level of reliability
we strive for. We are implementing the following preventions:

1. Enhanced monitoring for database query plan changes
2. Automated alerts for 10x performance degradation
3. Additional testing of database changes in production-like environment

Thank you for your patience and continued trust in our platform.
```

---

## Response Playbooks

### Playbook 1: High Error Rate (>5%)

**Symptoms**:
- Alert: "Error rate > 5%"
- Datadog shows red dashboard
- Customer support sees spike in reports

**Investigation**:
```bash
# Check which API endpoints are failing
SELECT endpoint, error_code, COUNT(*) as count
FROM request_logs
WHERE status >= 400
  AND timestamp > NOW() - INTERVAL '5 minutes'
GROUP BY endpoint, error_code
ORDER BY count DESC;

# Check recent deployments
git log --oneline | head -10

# Check for external service failures
curl https://status.stripe.example.com  # Payment processor
curl https://status.twilio.example.com  # SMS service
```

**Possible causes & fixes:**
1. **Code change caused exception**
   - Rollback recent deployment
   - Fix and redeploy

2. **Database query timeout**
   - Slow query kicked in after index change
   - Disable new index or optimize query
   - See RUNBOOK_DATABASE.md

3. **External service down**
   - Enable circuit breaker
   - Use fallback/cache
   - Wait for service to recover

4. **Configuration error**
   - Incorrect environment variable
   - Firewall rule change
   - Database connection string wrong

---

### Playbook 2: Payment Processor Down

**Symptoms**:
- Stripe/Paystack webhook not received
- Payment requests timeout
- "Cannot reach payment processor" errors

**Investigation**:
```bash
# Check processor status
curl https://status.stripe.example.com/api/v2/status.json | jq '.status'

# Check network connectivity
curl -v https://api.stripe.example.com/v1/account 2>&1 | grep -A5 'SSL\|TLS\|Connection'

# Check recent requests to processor
SELECT timestamp, processor, status, error
FROM processor_requests
WHERE timestamp > NOW() - INTERVAL '5 minutes'
ORDER BY timestamp DESC;
```

**Mitigation options:**
1. **Wait for recovery** (if processor temporarily down)
   - Hold payments in PROCESSING state
   - Retry automatically every 30 seconds
   - Set timeout to 5 minutes, then mark PENDING

2. **Enable backup processor**
   ```sql
   UPDATE system_config
   SET primary_processor = 'PAYSTACK', backup_processor = 'STRIPE'
   WHERE config_key = 'payment_processor';

   -- Route new payments through Paystack
   ```

3. **Graceful degradation**
   - Only accept payment methods that don't require processor
   - Hold other payments with clear user messaging
   - Resume processing when processor recovers

**Customer communication:**
```
We're experiencing issues with our payment processor. Payments may be delayed.
Please do not refresh or retry your payment - we'll process it automatically
once the processor recovers. Typical resolution: 15-30 minutes.
```

---

### Playbook 3: Database Availability

**Symptoms**:
- "Connection refused" errors
- Requests timing out
- Queries very slow

**Investigation**:
```bash
# Check database status
psql -h db.fintech.example.com -c "SELECT 1;"

# Check active connections
psql -d fintech -c "SELECT count(*) FROM pg_stat_activity;"

# Check if replication is lagging
psql -d fintech -c "SELECT NOW() - pg_last_wal_receive_lsn() AS replication_lag;"

# Check database size
psql -d fintech -c "SELECT pg_size_pretty(pg_database_size('fintech'));"
```

**Possible causes & mitigations:**

1. **Database crashed**
   - Check database logs
   - Restart database (will cause ~30 second outage)
   - Verify backups are recent

2. **Replication lag**
   - Failing over to replica may lose recent data
   - Only failover if primary completely down

3. **Connection pool exhausted**
   - Kill idle connections >10 minutes old
   - Temporarily reduce connection pool max
   - Restart application servers

4. **Disk full**
   - Delete old logs: `rm /var/log/postgres/*.old`
   - Delete temporary files: `rm /tmp/pg*`
   - Expand disk (requires downtime)

---

### Playbook 4: Service Deployment Failure

**Symptoms**:
- Deployment completes but service fails
- Requests return 5XX errors
- Health check failing

**Mitigation**:
```bash
# Rollback to previous version
git revert <commit-hash>
git push origin main

# Or rollback deployment
kubectl rollout undo deployment/payment-service

# Or restart from last working state
docker restart payment-service:v1.2.3

# Verify health
curl http://payment-service:8080/health
```

**Investigation** (after rollback):
1. Check deployment logs
2. Check application startup logs
3. Verify configuration/environment variables
4. Test in staging before redeploying

---

### Playbook 5: DDoS / Unusual Traffic Spike

**Symptoms**:
- Traffic suddenly 10x normal
- Specific IP addresses or user accounts sending lots of requests
- Resource usage maxed out

**Immediate mitigation:**
```
1. Enable DDoS protection
   - Cloudflare / AWS Shield
   - Rate limiting
   - WAF rules

2. Identify attacker sources
   SELECT source_ip, COUNT(*) as requests
   FROM request_logs
   WHERE timestamp > NOW() - INTERVAL '5 minutes'
   GROUP BY source_ip
   ORDER BY requests DESC;

3. Block malicious sources
   - IP blocklist rules
   - User account suspension
   - Geographic restrictions if needed

4. Gracefully degrade
   - Enable request queuing
   - Increase timeouts
   - Drop non-critical requests
```

**Communication:**
```
We are experiencing elevated traffic and may have temporarily degraded performance.
We are actively mitigating the issue. Thank you for your patience.
```

---

## Post-Incident Review

### Purpose
- **Learn from incidents** to prevent recurrence
- **Improve processes** based on what we learn
- **No blame** - focus on system and process, not individual
- **Action items** for team to implement

### Timing
- **SEV-1**: Review within 24 hours
- **SEV-2**: Review within 48 hours
- **SEV-3**: Review within 1 week
- **SEV-4**: Optional, if systematic issue

### Attendees
- Incident commander
- Technical lead who diagnosed
- Tech lead who implemented fix
- Incident reporters/on-call
- Engineering manager
- (Optional) CTO/VP for SEV-1

### Meeting Format (60 minutes)

**1. Timeline (15 minutes)**
```
Create complete timeline with facts (not opinions yet)

10:15:30 - Alert fires: "Error rate > 5%"
10:16:00 - Incident detected by monitoring
10:18:00 - Engineer acknowledges alert
10:25:00 - Incident commander assigned
10:30:00 - Root cause hypothesis: database index change
10:35:00 - Confirmed hypothesis with query analysis
10:40:00 - Fix deployed to staging
10:45:00 - Fix validated in staging
10:50:00 - Fix deployed to production
10:55:00 - Error rate returned to normal
11:05:00 - Stability confirmed

Total duration: 50 minutes
Time to diagnosis: 20 minutes
Time to fix: 35 minutes
```

**2. Root Cause Analysis (15 minutes)**
```
What happened?
- Query plan changed due to new index
- Changed from efficient plan (100ms) to inefficient plan (800ms)
- Caused timeout, which caused error

Why did it happen?
- Recent index added by migration
- Index statistics not updated
- Query optimizer chose suboptimal plan

Why didn't we catch it?
- No query plan monitoring
- Index change not tested in prod-like environment
- Gradual rollout would have caught it earlier

Why did it affect customers?
- ~15% of transactions were slow
- System timeout caused error instead of fallback

How can we prevent?
[See action items below]
```

**3. Impact Assessment (10 minutes)**
```
Customers affected: ~500
Transactions delayed: ~27,500
Transactions failed: ~200
Failed customer value: ~$50K (estimated)
Merchant complaints: 3

Severity: SEV-2 - Justified
```

**4. What Went Well (5 minutes)**
```
- Fast alert response (1 minute detection)
- Clear incident declaration in Slack
- Good coordination between backend and DBA teams
- Rollback would have been quick option if fix didn't work
- Transparent communication to customers
```

**5. What Could Be Better (5 minutes)**
```
- Delayed diagnosis (20 minutes) - could have been 5
  â†’ Need better query plan monitoring tools

- Too many people in incident channel causing noise
  â†’ Need clearer role separation

- Customer communication delayed (40 minutes)
  â†’ Need automatic status page update from monitoring
```

**6. Action Items (10 minutes)**

Format: [Team] [Action] [Owner] [Due date]

```
[ ] [Backend] Implement query plan monitoring dashboard
    Owner: @dba-team | Due: 2024-02-01

[ ] [DevOps] Test all DB migrations in production-like environment
    Owner: @devops-lead | Due: 2024-02-01

[ ] [Engineering] Add index hint hints to payment queries
    Owner: @jane.smith | Due: 2024-01-25

[ ] [Monitoring] Create alert for 10x+ query time degradation
    Owner: @monitoring-team | Due: 2024-02-01

[ ] [Process] Create runbook for "Slow database queries"
    Owner: @john.doe | Due: 2024-01-30

[ ] [Process] Implement automated status page updates from alerts
    Owner: @product-team | Due: 2024-02-15
```

---

### Post-Incident Review Template

**Template file**: POST_INCIDENT_REVIEW_TEMPLATE.md (see separate file)

---

## Escalation Matrix

```
SITUATION                          â†’ ESCALATE TO
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
No response from on-call           â†’ On-call manager
after 5 minutes (SEV-1)

Issue not identified after         â†’ Engineering manager
15 minutes of investigation (SEV-2)

Issue not resolved after           â†’ VP Engineering
2 hours (SEV-1) or 4 hours (SEV-2)

Disagreement on mitigation         â†’ Engineering manager
approach                           (or VP if manager is in incident)

Need customer communication         â†’ Product manager
approval                           + Legal review if data loss

Need to take customer data offline â†’ CTO + General Counsel
for security investigation

Financial impact >$100K            â†’ CFO
```

---

## Training & Certification

### Incident Commander Certification
All engineering managers and senior engineers should be certified to run incidents.

**Training:**
- 4-hour course covering this guide
- Role-playing 3 simulated incidents
- Pass quiz covering processes

**Recertification:** Annually or after major process changes

### On-Call Engineer Requirements
- Understand systems they're on-call for
- Can diagnose basic issues
- Know who to escalate to
- Can communicate status clearly

---

## Tools & Resources

**Incident Tracking**:
- Jira issue with `incident` label
- Link from Slack to ticket

**Communication**:
- Slack channels (public + private)
- Email for formal notifications
- Status page (status.fintech.example.com)

**Monitoring & Alerting**:
- Prometheus for metrics
- Datadog for dashboards
- AlertManager for notifications

**Documentation**:
- Runbooks (RUNBOOKS_GUIDE.md)
- Architecture diagrams (ARCHITECTURE.md)
- API docs (API_REFERENCE.md)

---

## Last Updated
2024-01-15

## Owner
Engineering Leadership Team

